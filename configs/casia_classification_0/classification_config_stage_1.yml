# training params
log_checkpoint: 10  # save checkpoint every epoch % log_checkpoint, if 0, save every epoch, just last checkpoint
start_epoch: 0
end_epoch: 8
verbose: False

resume: null # if you want to resume your model from checkpoint, provide absolute path
freeze: null  # if you want to freeze your layers during training (fine tune case)
# example
# first 11 epochs will be frozen everything, except last layer
# epochs 11-13 will be frozen everything, except last 2 layers
# on epoch 14 will be frozen everything, except last 3 layers
# on epoch 15 will be frozen only last 3 layers
# please, provide epochs in ascending order
#freeze:
#  - 10: 1
#  - 13: 2
#  - 14: [0, 1, 2, 3]  # may be 0
#  - 15: [-1, -2, -3]


# loss params
loss_name: CrossEntropyLoss
loss_params: null

# lr scheduler params
lr_scheduler_name: MultiStepLR
lr_scheduler_params:
  milestones:
    - 4
    - 10
    - 16
  gamma: 0.5

# optimizer params
optimizer_name: RAdam
optimizer_params:
  lr: 0.001