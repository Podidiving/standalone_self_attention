# general params
device: cuda:0
trace: null  # if not null then path, where to save traced model
checkpoint_out: null  # path, where to store your checkpoints
log_file: null  # if you want logs to be saved to file, provide absolute path

# configs for dataloader
image_size: 32
image_path_column: filepath
target_column: label

# configs for dataloader (train)
root_prefix_train: null
in_csv_train: null
batch_size_train: 64
shuffle_train: True

# configs for dataloader (test)
root_prefix_test: null
in_csv_test: null
batch_size_test: 128
shuffle_test: False


# configs for creating model
resnet_num: 26  # one of 26, 38, 50
resnet_type: attention  # one of attn, plain
num_classes: 10
in_places: 64  # keep default
expansion: 4  # keep default
stem_spatial_downsample: soft  # hard (224x224) or soft (32x32)
# if you want to customize your model
block_name: null
num_blocks: null
stem: False,


# training params
log_checkpoint: 15  # save checkpoint every epoch % log_checkpoint, if 0, save every epoch, just last checkpoint
start_epoch: 0
end_epoch: 100
verbose: False

resume: null  # if you want to resume your model from checkpoint, provide absolute path
freeze: null  # if you want to freeze your layers during training (fine tune case)
# example
# first 11 epochs will be frozen everything, except last layer
# epochs 11-13 will be frozen everything, except last 2 layers
# on epoch 14 will be frozen everything, except last 3 layers
# on epoch 15 will be frozen only last 3 layers
# please, provide epochs in ascending order
#freeze:
#  - 10: 1
#  - 13: 2
#  - 14: [0, 1, 2, 3]  # may be 0
#  - 15: [-1, -2, -3]


# loss params
loss_name: CrossEntropyLoss
loss_params: null

# lr scheduler params
lr_scheduler_name: ReduceLROnPlateau
lr_scheduler_params: null

# optimizer params
optimizer_name: Adam
optimizer_params: null

# visualization (with Visdom)
visdom_port: 6006
visdom_log_file: null
visdom_env_name: null

# logging
log_batch_interval: 10  # How often log metrics